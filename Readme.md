# Caprae Capital: Lead Generation with AI

## Overview

This project demonstrates a scalable, ethical, and business-focused approach to company data enrichment and lead generation for Caprae Capital. It simulates web scraping, enriches company profiles with actionable insights, and provides a fast, interactive Streamlit dashboard for searching and exploring company data.

---

## Proposed Features 

- **Simulated Scraping & Enrichment:** Extracts company name, website, industry, specialties, headquarters, overview, quick links, and contact info from local/cached data. (**Achieved**)
- **Contact Info Extraction:** Uses regex and logic to find public emails, phone numbers, and locations from company contact pages. (**Achieved**)
- **Quick Links:** Identifies and displays main navigation links (About, Contact, Blog, Investors, etc.) for each company. (**Achieved**)
- **Streamlit Dashboard:** Search companies by name or industry/specialties, view enriched details, and access clickable quick links and contact info. (**Achieved**)
- **(Optional) NLP Summarization:** Ready for integration with NLP models to summarize company overviews. (**Not Achieved due to time constraint.**)

---

## Setup Instructions

1. **Clone the repository:**
   ```sh
   git clone <your-repo-url>
   cd Caprae Capital
   ```

2. **Create and activate a virtual environment (optional but recommended):**
   ```sh
   python -m venv venv
   # On Windows:
   .\venv\Scripts\activate
   # On macOS/Linux:
   source venv/bin/activate
   ```

3. **Install dependencies:**
   ```sh
   pip install -r requirements.txt
   ```

4. **Generate the dataset (if `data.csv` is not present):**
   - Open and run all cells in `backend.ipynb` to create `data.csv`.

5. **Run the Streamlit app:**
   ```sh
   streamlit run app.py
   ```

---

## File Structure

- `app.py` — Streamlit dashboard for searching and viewing company data.
- `data.csv` — Enriched company dataset (simulated scraping output). Can be generated by running backend.ipynb
- `Report.md` — One-page report explaining rationale, model selection, and approach.
- `requirements.txt` — Python dependencies.
- `backend.ipynb` — Jupyter notebook for backend scraping/enrichment logic.
- `Readme.md` — This file.

---

## Usage

- Launch the Streamlit app.
- Use the sidebar to search companies by name or industry/specialties.
- Click on a company to view detailed info, quick links, and contact details.
- All data is simulated for ethical compliance and reproducibility.

---

## Technologies Used

- **Python 3.x**
- **Selenium** (web scraping simulation)
- **pandas** (data manipulation)
- **Streamlit** (dashboard/UI)
- **regex** (contact info extraction)
- **ast** (parsing stringified dictionaries)
- **(Optional) transformers** (for NLP summarization)

---

## Notes

- No live scraping of restricted sources (e.g., LinkedIn) was performed.
- The pipeline is designed for easy extension with real scraping.
- For demo purposes, all data is simulated or cached.

---

## Author

Vijay Bahadur Vishwakarma

---

## License

This project is for demonstration and educational purposes only.